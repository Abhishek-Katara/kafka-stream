{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create Staging Delta Table"
      ],
      "metadata": {
        "id": "2Kfi7hMXKvP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "\n",
        "CREATE TABLE kafka_data_stg (\n",
        "    STOCK_NAME STRING,\n",
        "    CURR_VALUE FLOAT,\n",
        "    CLOSE_VALUE FLOAT,\n",
        "    CURRENT_TIME STRING\n",
        ") USING DELTA"
      ],
      "metadata": {
        "id": "uNgQ04buKx9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create PySpark Streaming Job to load data into Delta table"
      ],
      "metadata": {
        "id": "9nub6IyhLO7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import split, col, from_unixtime\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"Kafka\").getOrCreate()\n",
        "\n",
        "# Read data from Kafka\n",
        "df = (spark.readStream\n",
        "  .format(\"kafka\")\n",
        "  .option(\"kafka.bootstrap.servers\", \"local_host:9092\")\n",
        "  .option(\"subscribe\", \"kafka-topic\")\n",
        "  .option(\"startingOffsets\", \"latest\")\n",
        "  .load())\n",
        "\n",
        "# Split the value column by comma and select the resulting columns\n",
        "split_df = df.select(split(col(\"value\"), \",\").alias(\"split_values\"))\n",
        "\n",
        "# Extract individual columns from the split array\n",
        "parsed_df = split_df.select(\n",
        "    col(\"split_values\")[0].alias(\"STOCK_NAME\"),\n",
        "    col(\"split_values\")[1].cast(\"FLOAT\").alias(\"CURR_VALUE\"),\n",
        "    col(\"split_values\")[2].cast(\"FLOAT\").alias(\"CLOSE_VALUE\"),\n",
        "    col(\"split_values\")[3].alias(\"CURRENT_TIME\")\n",
        ")\n",
        "\n",
        "# Write the data to the Databricks table\n",
        "query = parsed_df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"delta\") \\\n",
        "    .option(\"checkpointLocation\", \"/tmp\") \\\n",
        "    .table(\"kafka_data_stg\")\n",
        "\n",
        "query.awaitTermination()"
      ],
      "metadata": {
        "id": "v4YydeY9K8go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Warehouse Table for Complex Trasformation and Analytics"
      ],
      "metadata": {
        "id": "RlOc7guqLZVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "\n",
        "CREATE TABLE kafka_data (\n",
        "    STOCK_NAME STRING,\n",
        "    CURR_VALUE FLOAT,\n",
        "    CLOSE_VALUE FLOAT,\n",
        "    DIFF_VALUE FLOAT AS ROUND(CAST(CURR_VALUE AS FLOAT)-CAST(CLOSE_VALUE AS FLOAT), 2),\n",
        "    PER_VALUE FLOAT AS ((ROUND(CAST(CURR_VALUE AS FLOAT)-CAST(CLOSE_VALUE AS FLOAT), 2))/CAST(CLOSE_VALUE AS FLOAT))*100,\n",
        "    CURRENT_TIME TIMESTAMP\n",
        ")\n",
        "COPY INTO kafka_data_stg\n",
        "FROM delta_table\n",
        "DELIMITER ','"
      ],
      "metadata": {
        "id": "0E1r4rxWLqOb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
